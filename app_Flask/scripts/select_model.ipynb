{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\debac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\debac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Date             User  \\\n",
      "0  2019-05-18 23:50:47+00:00  advosushildixit   \n",
      "1  2019-05-18 23:00:03+00:00           jiaeur   \n",
      "2  2019-05-18 22:53:54+00:00    PVenkatGandhi   \n",
      "3  2019-05-18 22:20:48+00:00      TheNirbhay1   \n",
      "4  2019-05-18 21:22:29+00:00      ShakeChilli   \n",
      "\n",
      "                                               Tweet Emotion  \n",
      "0  @anjanaomkashyap I am seeing you as future #bj...     neg  \n",
      "1  #LokSabhaElections2019 \\n23rd May 2019 will re...     neg  \n",
      "2  #LokSabhaElections2019 \\n23rd May 2019 will re...     neg  \n",
      "3  PM Modi creates a new record of being the only...     pos  \n",
      "4  @abhijitmajumder Appointment of Successor! \\n\\...     pos  \n",
      "                        Date             User  \\\n",
      "0  2019-05-18 19:49:52+00:00     Sunnysweet16   \n",
      "1  2019-05-18 18:56:52+00:00    drnitinchaube   \n",
      "2  2019-05-18 18:54:01+00:00        mrvivek07   \n",
      "3  2019-05-18 18:52:03+00:00    JosephPravinP   \n",
      "4  2019-05-18 18:31:10+00:00  VandanaMegastar   \n",
      "\n",
      "                                               Tweet Emotion  \n",
      "0  Wonder why no academic or journalist asks INC ...     pos  \n",
      "1  Congrats for the change #australiavotes2019 an...     pos  \n",
      "2  Peopel Say “Govt Ne 70 Years Kya kiya”.\\nUnse ...     neg  \n",
      "3  @ajaymaken @RahulGandhi And as a final touch, ...     pos  \n",
      "4  #LokSabhaElections2019 Anyone not having mass ...     pos  \n"
     ]
    }
   ],
   "source": [
    "df_modi = pd.read_csv(\"../datasets/ModiRelatedTweetsWithSentiment.csv\")\n",
    "df_rahul = pd.read_csv(\"../datasets/RahulRelatedTweetsWithSentiment.csv\")\n",
    "df_modi = df_modi.drop(df_modi.columns[0], axis=1)\n",
    "df_rahul = df_rahul.drop(df_rahul.columns[0], axis=1)\n",
    "\n",
    "print(df_modi.head())\n",
    "print(df_rahul.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Check if text is a string\n",
    "    if isinstance(text, str):\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        # Remove emojis (if needed)\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "        # Tokenization and removal of stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in text.split() if word not in stop_words]\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        return \" \".join(words)\n",
    "    else:\n",
    "        return \"\"  # Return empty string for non-string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text preprocessing to each tweet\n",
    "df_modi['Cleaned Tweet'] = df_modi['Tweet'].fillna('').apply(preprocess_text)\n",
    "df_rahul['Cleaned Tweet'] = df_rahul['Tweet'].fillna('').apply(preprocess_text)\n",
    "\n",
    "# Encode sentiment labels (positive: 1, negative: 0)\n",
    "df_modi['Encoded Emotion'] = df_modi['Emotion'].map({'pos': 1, 'neg': 0})\n",
    "df_rahul['Encoded Emotion'] = df_rahul['Emotion'].map({'pos': 1, 'neg': 0})\n",
    "\n",
    "#print(df_modi.head())\n",
    "#print(df_rahul.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Date             User  \\\n",
      "0  2019-05-18 23:50:47+00:00  advosushildixit   \n",
      "1  2019-05-18 23:00:03+00:00           jiaeur   \n",
      "2  2019-05-18 22:53:54+00:00    PVenkatGandhi   \n",
      "3  2019-05-18 22:20:48+00:00      TheNirbhay1   \n",
      "4  2019-05-18 21:22:29+00:00      ShakeChilli   \n",
      "\n",
      "                                               Tweet Emotion  \\\n",
      "0  @anjanaomkashyap I am seeing you as future #bj...     neg   \n",
      "1  #LokSabhaElections2019 \\n23rd May 2019 will re...     neg   \n",
      "2  #LokSabhaElections2019 \\n23rd May 2019 will re...     neg   \n",
      "3  PM Modi creates a new record of being the only...     pos   \n",
      "4  @abhijitmajumder Appointment of Successor! \\n\\...     pos   \n",
      "\n",
      "                                       Cleaned Tweet  Encoded Emotion Source  \n",
      "0  anjanaomkashyap seeing future bjp spokesperson...              0.0   Modi  \n",
      "1  loksabhaelections2019 23rd may 2019 reveal eve...              0.0   Modi  \n",
      "2  loksabhaelections2019 23rd may 2019 reveal eve...              0.0   Modi  \n",
      "3  pm modi creates new record pm democratic count...              1.0   Modi  \n",
      "4  abhijitmajumder appointment successor god forb...              1.0   Modi  \n"
     ]
    }
   ],
   "source": [
    "df_modi['Source'] = 'Modi'\n",
    "df_rahul['Source'] = 'Rahul'\n",
    "df_combined = pd.concat([df_modi, df_rahul], ignore_index=True)\n",
    "df_combined.dropna(inplace=True)\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38239    shashitharoor atleast rahulgandhi couldve forw...\n",
      "17462    smart utilization communication channel touch ...\n",
      "15440    campaign loksabhaelections2019 coming end toda...\n",
      "30419    pun_starr irony many people confused rahulgand...\n",
      "22459    visited 200 house last 2 day love n affection ...\n",
      "Name: Cleaned Tweet, dtype: object\n",
      "38239    0.0\n",
      "17462    1.0\n",
      "15440    1.0\n",
      "30419    0.0\n",
      "22459    1.0\n",
      "Name: Encoded Emotion, dtype: float64\n",
      "6524     modi thanos gathbandhan godamn avenger releasi...\n",
      "30909    mkstalin addressal kanyakumari said rahul gand...\n",
      "36466    already u lost seat madhubani loksabhaelection...\n",
      "9801     one way modi saying anyone fight corruption ch...\n",
      "25645     thank ani narendramodi ji interview narendramodi\n",
      "Name: Cleaned Tweet, dtype: object\n",
      "6524     1.0\n",
      "30909    0.0\n",
      "36466    0.0\n",
      "9801     0.0\n",
      "25645    1.0\n",
      "Name: Encoded Emotion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_combined['Cleaned Tweet'], df_combined['Encoded Emotion'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_train.head())\n",
    "print(X_test.head())\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(31953, 5000)\n",
      "(7989, 5000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(type(X_train_tfidf))\n",
    "print(type(X_test_tfidf))\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with parameters: 0.001 l1 100\n",
      "Parameters: 0.001 l1 100\n",
      "Accuracy: 0.5786706721742396\n",
      "Training model with parameters: 0.001 l1 1000\n",
      "Parameters: 0.001 l1 1000\n",
      "Accuracy: 0.5786706721742396\n",
      "Training model with parameters: 0.001 l1 10000\n",
      "Parameters: 0.001 l1 10000\n",
      "Accuracy: 0.5786706721742396\n",
      "Training model with parameters: 0.001 l2 100\n",
      "Parameters: 0.001 l2 100\n",
      "Accuracy: 0.5786706721742396\n",
      "Training model with parameters: 0.001 l2 1000\n",
      "Parameters: 0.001 l2 1000\n",
      "Accuracy: 0.5786706721742396\n",
      "Training model with parameters: 0.001 l2 10000\n",
      "Parameters: 0.001 l2 10000\n",
      "Accuracy: 0.5786706721742396\n",
      "Training model with parameters: 0.01 l1 100\n",
      "Parameters: 0.01 l1 100\n",
      "Accuracy: 0.5836775566403806\n",
      "Training model with parameters: 0.01 l1 1000\n",
      "Parameters: 0.01 l1 1000\n",
      "Accuracy: 0.5836775566403806\n",
      "Training model with parameters: 0.01 l1 10000\n",
      "Parameters: 0.01 l1 10000\n",
      "Accuracy: 0.5836775566403806\n",
      "Training model with parameters: 0.01 l2 100\n",
      "Parameters: 0.01 l2 100\n",
      "Accuracy: 0.6178495431217924\n",
      "Training model with parameters: 0.01 l2 1000\n",
      "Parameters: 0.01 l2 1000\n",
      "Accuracy: 0.6178495431217924\n",
      "Training model with parameters: 0.01 l2 10000\n",
      "Parameters: 0.01 l2 10000\n",
      "Accuracy: 0.6178495431217924\n",
      "Training model with parameters: 0.1 l1 100\n",
      "Parameters: 0.1 l1 100\n",
      "Accuracy: 0.6876955814244586\n",
      "Training model with parameters: 0.1 l1 1000\n",
      "Parameters: 0.1 l1 1000\n",
      "Accuracy: 0.6876955814244586\n",
      "Training model with parameters: 0.1 l1 10000\n",
      "Parameters: 0.1 l1 10000\n",
      "Accuracy: 0.6876955814244586\n",
      "Training model with parameters: 0.1 l2 100\n",
      "Parameters: 0.1 l2 100\n",
      "Accuracy: 0.726749280260358\n",
      "Training model with parameters: 0.1 l2 1000\n",
      "Parameters: 0.1 l2 1000\n",
      "Accuracy: 0.726749280260358\n",
      "Training model with parameters: 0.1 l2 10000\n",
      "Parameters: 0.1 l2 10000\n",
      "Accuracy: 0.726749280260358\n",
      "Training model with parameters: 1 l1 100\n",
      "Parameters: 1 l1 100\n",
      "Accuracy: 0.7539116284891726\n",
      "Training model with parameters: 1 l1 1000\n",
      "Parameters: 1 l1 1000\n",
      "Accuracy: 0.7539116284891726\n",
      "Training model with parameters: 1 l1 10000\n",
      "Parameters: 1 l1 10000\n",
      "Accuracy: 0.7539116284891726\n",
      "Training model with parameters: 1 l2 100\n",
      "Parameters: 1 l2 100\n",
      "Accuracy: 0.753285767930905\n",
      "Training model with parameters: 1 l2 1000\n",
      "Parameters: 1 l2 1000\n",
      "Accuracy: 0.753285767930905\n",
      "Training model with parameters: 1 l2 10000\n",
      "Parameters: 1 l2 10000\n",
      "Accuracy: 0.753285767930905\n",
      "Training model with parameters: 10 l1 100\n",
      "Parameters: 10 l1 100\n",
      "Accuracy: 0.7516585304794092\n",
      "Training model with parameters: 10 l1 1000\n",
      "Parameters: 10 l1 1000\n",
      "Accuracy: 0.7516585304794092\n",
      "Training model with parameters: 10 l1 10000\n",
      "Parameters: 10 l1 10000\n",
      "Accuracy: 0.7516585304794092\n",
      "Training model with parameters: 10 l2 100\n",
      "Parameters: 10 l2 100\n",
      "Accuracy: 0.7542871448241332\n",
      "Training model with parameters: 10 l2 1000\n",
      "Parameters: 10 l2 1000\n",
      "Accuracy: 0.7542871448241332\n",
      "Training model with parameters: 10 l2 10000\n",
      "Parameters: 10 l2 10000\n",
      "Accuracy: 0.7542871448241332\n",
      "Training model with parameters: 100 l1 100\n",
      "Parameters: 100 l1 100\n",
      "Accuracy: 0.7448992364501189\n",
      "Training model with parameters: 100 l1 1000\n",
      "Parameters: 100 l1 1000\n",
      "Accuracy: 0.7447740643384654\n",
      "Training model with parameters: 100 l1 10000\n",
      "Parameters: 100 l1 10000\n",
      "Accuracy: 0.7447740643384654\n",
      "Training model with parameters: 100 l2 100\n",
      "Parameters: 100 l2 100\n",
      "Accuracy: 0.7467768181249218\n",
      "Training model with parameters: 100 l2 1000\n",
      "Parameters: 100 l2 1000\n",
      "Accuracy: 0.7467768181249218\n",
      "Training model with parameters: 100 l2 10000\n",
      "Parameters: 100 l2 10000\n",
      "Accuracy: 0.7467768181249218\n",
      "Best parameters: (10, 'l2', 100)\n",
      "Best accuracy: 0.7542871448241332\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from math import prod\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a list of parameter combinations to test\n",
    "C= [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "penalty= ['l1', 'l2']\n",
    "max_iter= [100, 1000, 10000]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "# Train multiple models with different parameter combinations\n",
    "for C, penalty, max_iter in product(C, penalty, max_iter):   \n",
    "    print(\"Training model with parameters:\", C, penalty, max_iter)\n",
    "    clf = LogisticRegression(C=C, penalty=penalty, max_iter=max_iter, solver='liblinear')\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Parameters:\", C, penalty, max_iter)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Update best parameters if the current model has higher accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = (C, penalty, max_iter)\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best accuracy:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with parameters: 10 rbf\n",
      "Parameters: 10 rbf\n",
      "Accuracy: 0.7897108524220804\n",
      "Best Parameters: {'C': 10, 'kernel': 'rbf'}\n",
      "Best Accuracy: 0.7897108524220804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "C_values = [10]\n",
    "kernel = ['rbf']\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for C, kernel in product(C_values, kernel):\n",
    "    print(\"Training model with parameters:\", C, kernel)\n",
    "    clf = SVC(C=C, kernel=kernel)\n",
    "    clf.fit(X_train_tfidf, y_train)  # Train the model on the training data\n",
    "    y_pred = clf.predict(X_test_tfidf)  # Predict the labels for the validation set\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Calculate the accuracy on the validation set\n",
    "    print(\"Parameters:\", C, kernel)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    # Check if the current model has the highest accuracy so far\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = {'C': C, 'kernel': kernel}\n",
    "\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82      4623\n",
      "         1.0       0.77      0.71      0.74      3366\n",
      "\n",
      "    accuracy                           0.79      7989\n",
      "   macro avg       0.79      0.78      0.78      7989\n",
      "weighted avg       0.79      0.79      0.79      7989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming that `y_test` are your true labels and `model` is your trained model\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with parameters: 50 None 2 1\n",
      "Parameters: 50 None 2 1\n",
      "Accuracy: 0.7612967830767305\n",
      "Training model with parameters: 50 None 2 2\n",
      "Parameters: 50 None 2 2\n",
      "Accuracy: 0.7470271623482289\n",
      "Training model with parameters: 50 None 2 4\n",
      "Parameters: 50 None 2 4\n",
      "Accuracy: 0.7268744523720115\n",
      "Training model with parameters: 50 None 5 1\n",
      "Parameters: 50 None 5 1\n",
      "Accuracy: 0.7605457504068094\n",
      "Training model with parameters: 50 None 5 2\n",
      "Parameters: 50 None 5 2\n",
      "Accuracy: 0.7441482037801977\n",
      "Training model with parameters: 50 None 5 4\n",
      "Parameters: 50 None 5 4\n",
      "Accuracy: 0.7306296157216172\n",
      "Training model with parameters: 50 None 10 1\n",
      "Parameters: 50 None 10 1\n",
      "Accuracy: 0.7566654149455502\n",
      "Training model with parameters: 50 None 10 2\n",
      "Parameters: 50 None 10 2\n",
      "Accuracy: 0.748028539241457\n",
      "Training model with parameters: 50 None 10 4\n",
      "Parameters: 50 None 10 4\n",
      "Accuracy: 0.7326323695080736\n",
      "Training model with parameters: 50 10 2 1\n",
      "Parameters: 50 10 2 1\n",
      "Accuracy: 0.6165978220052573\n",
      "Training model with parameters: 50 10 2 2\n",
      "Parameters: 50 10 2 2\n",
      "Accuracy: 0.6150957566654149\n",
      "Training model with parameters: 50 10 2 4\n",
      "Parameters: 50 10 2 4\n",
      "Accuracy: 0.6105895606458881\n",
      "Training model with parameters: 50 10 5 1\n",
      "Parameters: 50 10 5 1\n",
      "Accuracy: 0.6104643885342346\n",
      "Training model with parameters: 50 10 5 2\n",
      "Parameters: 50 10 5 2\n",
      "Accuracy: 0.6084616347477781\n",
      "Training model with parameters: 50 10 5 4\n",
      "Parameters: 50 10 5 4\n",
      "Accuracy: 0.6120916259857304\n",
      "Training model with parameters: 50 10 10 1\n",
      "Parameters: 50 10 10 1\n",
      "Accuracy: 0.6139692076605332\n",
      "Training model with parameters: 50 10 10 2\n",
      "Parameters: 50 10 10 2\n",
      "Accuracy: 0.6114657654274628\n",
      "Training model with parameters: 50 10 10 4\n",
      "Parameters: 50 10 10 4\n",
      "Accuracy: 0.612717486543998\n",
      "Training model with parameters: 50 20 2 1\n",
      "Parameters: 50 20 2 1\n",
      "Accuracy: 0.662911503317061\n",
      "Training model with parameters: 50 20 2 2\n",
      "Parameters: 50 20 2 2\n",
      "Accuracy: 0.663161847540368\n",
      "Training model with parameters: 50 20 2 4\n",
      "Parameters: 50 20 2 4\n",
      "Accuracy: 0.6640380523219427\n",
      "Training model with parameters: 50 20 5 1\n",
      "Parameters: 50 20 5 1\n",
      "Accuracy: 0.6651646013268244\n",
      "Training model with parameters: 50 20 5 2\n",
      "Parameters: 50 20 5 2\n",
      "Accuracy: 0.6615346100888722\n",
      "Training model with parameters: 50 20 5 4\n",
      "Parameters: 50 20 5 4\n",
      "Accuracy: 0.6594066841907623\n",
      "Training model with parameters: 50 20 10 1\n",
      "Parameters: 50 20 10 1\n",
      "Accuracy: 0.6641632244335962\n",
      "Training model with parameters: 50 20 10 2\n",
      "Parameters: 50 20 10 2\n",
      "Accuracy: 0.6597822005257229\n",
      "Training model with parameters: 50 20 10 4\n",
      "Parameters: 50 20 10 4\n",
      "Accuracy: 0.6610339216422582\n",
      "Training model with parameters: 50 30 2 1\n",
      "Parameters: 50 30 2 1\n",
      "Accuracy: 0.6983352109150082\n",
      "Training model with parameters: 50 30 2 2\n",
      "Parameters: 50 30 2 2\n",
      "Accuracy: 0.6871948929778445\n",
      "Training model with parameters: 50 30 2 4\n",
      "Parameters: 50 30 2 4\n",
      "Accuracy: 0.6819376642883965\n",
      "Training model with parameters: 50 30 5 1\n",
      "Parameters: 50 30 5 1\n",
      "Accuracy: 0.6908248842157967\n",
      "Training model with parameters: 50 30 5 2\n",
      "Parameters: 50 30 5 2\n",
      "Accuracy: 0.6895731630992615\n",
      "Training model with parameters: 50 30 5 4\n",
      "Parameters: 50 30 5 4\n",
      "Accuracy: 0.6853173113030416\n",
      "Training model with parameters: 50 30 10 1\n",
      "Parameters: 50 30 10 1\n",
      "Accuracy: 0.6940793591187884\n",
      "Training model with parameters: 50 30 10 2\n",
      "Parameters: 50 30 10 2\n",
      "Accuracy: 0.6849417949680812\n",
      "Training model with parameters: 50 30 10 4\n",
      "Parameters: 50 30 10 4\n",
      "Accuracy: 0.6843159344098135\n",
      "Training model with parameters: 100 None 2 1\n",
      "Parameters: 100 None 2 1\n",
      "Accuracy: 0.7630491926398798\n",
      "Training model with parameters: 100 None 2 2\n",
      "Parameters: 100 None 2 2\n",
      "Accuracy: 0.7487795719113781\n",
      "Training model with parameters: 100 None 2 4\n",
      "Parameters: 100 None 2 4\n",
      "Accuracy: 0.7326323695080736\n",
      "Training model with parameters: 100 None 5 1\n",
      "Parameters: 100 None 5 1\n",
      "Accuracy: 0.7602954061835023\n",
      "Training model with parameters: 100 None 5 2\n",
      "Parameters: 100 None 5 2\n",
      "Accuracy: 0.7514081862561022\n",
      "Training model with parameters: 100 None 5 4\n",
      "Parameters: 100 None 5 4\n",
      "Accuracy: 0.7290023782701214\n",
      "Training model with parameters: 100 None 10 1\n",
      "Parameters: 100 None 10 1\n",
      "Accuracy: 0.7596695456252347\n",
      "Training model with parameters: 100 None 10 2\n",
      "Parameters: 100 None 10 2\n",
      "Accuracy: 0.7495306045812993\n",
      "Training model with parameters: 100 None 10 4\n",
      "Parameters: 100 None 10 4\n",
      "Accuracy: 0.7286268619351608\n",
      "Training model with parameters: 100 10 2 1\n",
      "Parameters: 100 10 2 1\n",
      "Accuracy: 0.6130930028789585\n",
      "Training model with parameters: 100 10 2 2\n",
      "Parameters: 100 10 2 2\n",
      "Accuracy: 0.6084616347477781\n",
      "Training model with parameters: 100 10 2 4\n",
      "Parameters: 100 10 2 4\n",
      "Accuracy: 0.6144698961071473\n",
      "Training model with parameters: 100 10 5 1\n",
      "Parameters: 100 10 5 1\n",
      "Accuracy: 0.6107147327575416\n",
      "Training model with parameters: 100 10 5 2\n",
      "Parameters: 100 10 5 2\n",
      "Accuracy: 0.6149705845537614\n",
      "Training model with parameters: 100 10 5 4\n",
      "Parameters: 100 10 5 4\n",
      "Accuracy: 0.6110902490925022\n",
      "Training model with parameters: 100 10 10 1\n",
      "Parameters: 100 10 10 1\n",
      "Accuracy: 0.6148454124421079\n",
      "Training model with parameters: 100 10 10 2\n",
      "Parameters: 100 10 10 2\n",
      "Accuracy: 0.6122167980973839\n",
      "Training model with parameters: 100 10 10 4\n",
      "Parameters: 100 10 10 4\n",
      "Accuracy: 0.6113405933158093\n",
      "Training model with parameters: 100 20 2 1\n",
      "Parameters: 100 20 2 1\n",
      "Accuracy: 0.6702966579046189\n",
      "Training model with parameters: 100 20 2 2\n",
      "Parameters: 100 20 2 2\n",
      "Accuracy: 0.6617849543121792\n",
      "Training model with parameters: 100 20 2 4\n",
      "Parameters: 100 20 2 4\n",
      "Accuracy: 0.6597822005257229\n",
      "Training model with parameters: 100 20 5 1\n",
      "Parameters: 100 20 5 1\n",
      "Accuracy: 0.6695456252346977\n",
      "Training model with parameters: 100 20 5 2\n",
      "Parameters: 100 20 5 2\n",
      "Accuracy: 0.6667918387783202\n",
      "Training model with parameters: 100 20 5 4\n",
      "Parameters: 100 20 5 4\n",
      "Accuracy: 0.66003254474903\n",
      "Training model with parameters: 100 20 10 1\n",
      "Parameters: 100 20 10 1\n",
      "Accuracy: 0.6661659782200525\n",
      "Training model with parameters: 100 20 10 2\n",
      "Parameters: 100 20 10 2\n",
      "Accuracy: 0.666040806108399\n",
      "Training model with parameters: 100 20 10 4\n",
      "Parameters: 100 20 10 4\n",
      "Accuracy: 0.6601577168606835\n",
      "Training model with parameters: 100 30 2 1\n",
      "Parameters: 100 30 2 1\n",
      "Accuracy: 0.6974590061334335\n",
      "Training model with parameters: 100 30 2 2\n",
      "Parameters: 100 30 2 2\n",
      "Accuracy: 0.689197646764301\n",
      "Training model with parameters: 100 30 2 4\n",
      "Parameters: 100 30 2 4\n",
      "Accuracy: 0.6830642132932783\n",
      "Training model with parameters: 100 30 5 1\n",
      "Parameters: 100 30 5 1\n",
      "Accuracy: 0.6965828013518588\n",
      "Training model with parameters: 100 30 5 2\n",
      "Parameters: 100 30 5 2\n",
      "Accuracy: 0.6846914507447741\n",
      "Training model with parameters: 100 30 5 4\n",
      "Parameters: 100 30 5 4\n",
      "Accuracy: 0.6829390411816247\n",
      "Training model with parameters: 100 30 10 1\n",
      "Parameters: 100 30 10 1\n",
      "Accuracy: 0.6977093503567405\n",
      "Training model with parameters: 100 30 10 2\n",
      "Parameters: 100 30 10 2\n",
      "Accuracy: 0.6904493678808361\n",
      "Training model with parameters: 100 30 10 4\n",
      "Parameters: 100 30 10 4\n",
      "Accuracy: 0.6796845662786332\n",
      "Training model with parameters: 200 None 2 1\n",
      "Parameters: 200 None 2 1\n",
      "Accuracy: 0.7627988484165728\n",
      "Training model with parameters: 200 None 2 2\n",
      "Parameters: 200 None 2 2\n",
      "Accuracy: 0.7519088747027163\n",
      "Training model with parameters: 200 None 2 4\n",
      "Parameters: 200 None 2 4\n",
      "Accuracy: 0.7311303041682313\n",
      "Training model with parameters: 200 None 5 1\n",
      "Parameters: 200 None 5 1\n",
      "Accuracy: 0.7645512579797221\n",
      "Training model with parameters: 200 None 5 2\n",
      "Parameters: 200 None 5 2\n",
      "Accuracy: 0.7515333583677557\n",
      "Training model with parameters: 200 None 5 4\n",
      "Parameters: 200 None 5 4\n",
      "Accuracy: 0.729628238828389\n",
      "Training model with parameters: 200 None 10 1\n",
      "Parameters: 200 None 10 1\n",
      "Accuracy: 0.7639253974214545\n",
      "Training model with parameters: 200 None 10 2\n",
      "Parameters: 200 None 10 2\n",
      "Accuracy: 0.7495306045812993\n",
      "Training model with parameters: 200 None 10 4\n",
      "Parameters: 200 None 10 4\n",
      "Accuracy: 0.7310051320565778\n",
      "Training model with parameters: 200 10 2 1\n",
      "Parameters: 200 10 2 1\n",
      "Accuracy: 0.615596445112029\n",
      "Training model with parameters: 200 10 2 2\n",
      "Parameters: 200 10 2 2\n",
      "Accuracy: 0.6110902490925022\n",
      "Training model with parameters: 200 10 2 4\n",
      "Parameters: 200 10 2 4\n",
      "Accuracy: 0.6110902490925022\n",
      "Training model with parameters: 200 10 5 1\n",
      "Parameters: 200 10 5 1\n",
      "Accuracy: 0.6109650769808487\n",
      "Training model with parameters: 200 10 5 2\n",
      "Parameters: 200 10 5 2\n",
      "Accuracy: 0.6129678307673051\n",
      "Training model with parameters: 200 10 5 4\n",
      "Parameters: 200 10 5 4\n",
      "Accuracy: 0.6099637000876205\n",
      "Training model with parameters: 200 10 10 1\n",
      "Parameters: 200 10 10 1\n",
      "Accuracy: 0.612467142320691\n",
      "Training model with parameters: 200 10 10 2\n",
      "Parameters: 200 10 10 2\n",
      "Accuracy: 0.6165978220052573\n",
      "Training model with parameters: 200 10 10 4\n",
      "Parameters: 200 10 10 4\n",
      "Accuracy: 0.6087119789710852\n",
      "Training model with parameters: 200 20 2 1\n",
      "Parameters: 200 20 2 1\n",
      "Accuracy: 0.6649142571035174\n",
      "Training model with parameters: 200 20 2 2\n",
      "Parameters: 200 20 2 2\n",
      "Accuracy: 0.6656652897734385\n",
      "Training model with parameters: 200 20 2 4\n",
      "Parameters: 200 20 2 4\n",
      "Accuracy: 0.6564025535110777\n",
      "Training model with parameters: 200 20 5 1\n",
      "Parameters: 200 20 5 1\n",
      "Accuracy: 0.6687945925647766\n",
      "Training model with parameters: 200 20 5 2\n",
      "Parameters: 200 20 5 2\n",
      "Accuracy: 0.6649142571035174\n",
      "Training model with parameters: 200 20 5 4\n",
      "Parameters: 200 20 5 4\n",
      "Accuracy: 0.658154963074227\n",
      "Training model with parameters: 200 20 10 1\n",
      "Parameters: 200 20 10 1\n",
      "Accuracy: 0.6701714857929654\n",
      "Training model with parameters: 200 20 10 2\n",
      "Parameters: 200 20 10 2\n",
      "Accuracy: 0.6632870196520215\n",
      "Training model with parameters: 200 20 10 4\n",
      "Parameters: 200 20 10 4\n",
      "Accuracy: 0.6595318563024158\n",
      "Training model with parameters: 200 30 2 1\n",
      "Parameters: 200 30 2 1\n",
      "Accuracy: 0.6988358993616223\n",
      "Training model with parameters: 200 30 2 2\n",
      "Parameters: 200 30 2 2\n",
      "Accuracy: 0.6896983352109151\n",
      "Training model with parameters: 200 30 2 4\n",
      "Parameters: 200 30 2 4\n",
      "Accuracy: 0.6865690324195769\n",
      "Training model with parameters: 200 30 5 1\n",
      "Parameters: 200 30 5 1\n",
      "Accuracy: 0.6967079734635123\n",
      "Training model with parameters: 200 30 5 2\n",
      "Parameters: 200 30 5 2\n",
      "Accuracy: 0.6896983352109151\n",
      "Training model with parameters: 200 30 5 4\n",
      "Parameters: 200 30 5 4\n",
      "Accuracy: 0.6848166228564276\n",
      "Training model with parameters: 200 30 10 1\n",
      "Parameters: 200 30 10 1\n",
      "Accuracy: 0.6958317686819376\n",
      "Training model with parameters: 200 30 10 2\n",
      "Parameters: 200 30 10 2\n",
      "Accuracy: 0.6919514332206784\n",
      "Training model with parameters: 200 30 10 4\n",
      "Parameters: 200 30 10 4\n",
      "Accuracy: 0.6835649017398924\n",
      "Best Parameters: {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1}\n",
      "Best Accuracy: 0.7645512579797221\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "n_estimators = [50, 100, 200]\n",
    "max_depth = [None, 10, 20, 30]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for n_estimators, max_depth, min_samples_split, min_samples_leaf in product(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
    "    print(\"Training model with parameters:\", n_estimators, max_depth, min_samples_split, min_samples_leaf)\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "    clf.fit(X_train_tfidf, y_train)  # Train the model on the training data\n",
    "    y_pred = clf.predict(X_test_tfidf)  # Predict the labels for the validation set\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Calculate the accuracy on the validation set\n",
    "    print(\"Parameters:\", n_estimators, max_depth, min_samples_split, min_samples_leaf)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Check if the current model has the highest accuracy so far\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = {'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\debac\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with num_epochs=5 and batch_size=128\n",
      "Accuracy: 0.7775691747665405\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m5,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m80,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m101\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,241,505</span> (58.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,241,505\u001b[0m (58.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,080,501</span> (19.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,080,501\u001b[0m (19.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,161,004</span> (38.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m10,161,004\u001b[0m (38.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "embeddings_index = {}\n",
    "embedding_dim = 100  # Change this to match the embedding size you downloaded\n",
    "\n",
    "with open('../datasets/glove.twitter.27B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "\n",
    "# Define maximum number of words in each tweet and maximum word index\n",
    "MAX_WORDS = 280\n",
    "MAX_WORD_INDEX = 50000\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "#X_train_tfidf = X_train_tfidf.tolist()\n",
    "#X_test_tfidf = X_test_tfidf.tolist()\n",
    "tokenizer = Tokenizer(num_words=MAX_WORD_INDEX)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=MAX_WORDS)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=MAX_WORDS)\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((MAX_WORD_INDEX, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < MAX_WORD_INDEX:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in the embedding index will be all zeros\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=MAX_WORD_INDEX, output_dim=embedding_dim, weights=[embedding_matrix], input_length=MAX_WORDS, trainable=True))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with different parameters\n",
    "for num_epochs in [5]:\n",
    "    for batch_size in [128]:\n",
    "        print(f\"Training with num_epochs={num_epochs} and batch_size={batch_size}\")\n",
    "        model.fit(X_train_padded, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_test_padded, y_test), verbose=0)\n",
    "        _, accuracy = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 90ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.85      0.82      4623\n",
      "         1.0       0.77      0.68      0.72      3366\n",
      "\n",
      "    accuracy                           0.78      7989\n",
      "   macro avg       0.78      0.76      0.77      7989\n",
      "weighted avg       0.78      0.78      0.78      7989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test = np.array(y_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
