{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_keywords = {\n",
    "    \"bjp\": [\"bjp\", \"bharatiya janata party\", \"narendra\", \"modi\"],\n",
    "    \"inc\": [\"congress\", \"indian national congress\", \"rahul\", \"gandhi\"],\n",
    "    \"aap\": [\"aap\", \"aam aadmi party\", \"arvind\", \"kejriwal\"],\n",
    "    \"sp\": [\"sp\", \"samajwadi party\", \"akhilesh\", \"yadav\"],\n",
    "    \"bsp\": [\"bsp\", \"bahujan samaj party\", \"mayawati\"],\n",
    "    \"aitc\": [\"trinamool\", \"aitc\", \"mamata\", \"bannerjee\"],\n",
    "    \"cpim\": [\"cpim\", \"communist\"],\n",
    "    \"dmk\": [\"dmk\", \"south\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def identify_party(user_input):\n",
    "    for party, keywords in party_keywords.items():\n",
    "        for keyword in keywords:\n",
    "            pattern = re.compile(re.escape(keyword), re.IGNORECASE)\n",
    "            if pattern.search(user_input):\n",
    "                return party\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_manifesto_by_party(party):\n",
    "    manifesto_file_path = f\"../knowledge_base/{party.lower()}_manifesto.txt\"\n",
    "    try:\n",
    "        with open(manifesto_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            manifesto_text = file.read()\n",
    "        return manifesto_text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Manifesto for {party} not found.\")\n",
    "        return \"\"  # Return empty string if manifesto not found\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading manifesto for {party}: {str(e)}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\debac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def answer_query_based_on_manifesto(user_query, party_name):\n",
    "    # Load manifesto text\n",
    "    manifesto_text = load_manifesto_by_party(party_name)\n",
    "\n",
    "    if manifesto_text == \"Manifesto not found.\":\n",
    "        return \"Manifesto for the given party not found.\"\n",
    "\n",
    "    # Preprocess user query\n",
    "    user_query = user_query.lower()  # Convert to lowercase for case-insensitive matching\n",
    "\n",
    "    # Tokenize user query\n",
    "    query_tokens = nltk.word_tokenize(user_query)\n",
    "\n",
    "    # Tokenize manifesto text into sentences\n",
    "    manifesto_sentences = nltk.sent_tokenize(manifesto_text)\n",
    "\n",
    "    # Initialize list to store relevant sentences\n",
    "    relevant_sentences = []\n",
    "\n",
    "    # Calculate similarity scores between user query and each manifesto sentence\n",
    "    similarity_scores = []\n",
    "    for sentence in manifesto_sentences:\n",
    "        # Tokenize manifesto sentence\n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "        # Calculate Jaccard similarity between query and sentence tokens\n",
    "        similarity = len(set(query_tokens) & set(sentence_tokens)) / len(set(query_tokens) | set(sentence_tokens))\n",
    "        similarity_scores.append(similarity)\n",
    "\n",
    "        # If similarity score exceeds a threshold, consider the sentence relevant\n",
    "        if similarity > 0.5:  # Adjust the threshold as needed\n",
    "            relevant_sentences.append(sentence)\n",
    "\n",
    "    # If no relevant sentences are found\n",
    "    if not relevant_sentences:\n",
    "        return \"I couldn't find relevant information in the manifesto for the given query.\"\n",
    "\n",
    "    # Concatenate relevant sentences to form the response\n",
    "    response = \" \".join(relevant_sentences)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "import os\n",
    "\n",
    "manifesto_directory = \"../knowledge_base/\"\n",
    "\n",
    "party_manifestos = {}\n",
    "\n",
    "for filename in os.listdir(manifesto_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        party_name = os.path.splitext(filename)[0].replace(\"_manifesto\", \"\")\n",
    "        with open(os.path.join(manifesto_directory, filename), \"r\", encoding='utf_8') as file:\n",
    "            manifesto_text = file.read()\n",
    "        party_manifestos[party_name] = manifesto_text\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "class PartyManifestoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, party_manifestos, tokenizer):\n",
    "        self.party_manifestos = party_manifestos\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        party = list(self.party_manifestos.keys())[idx]\n",
    "        manifesto_text = self.party_manifestos[party]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            manifesto_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = self.tokenizer.encode_plus(\n",
    "            manifesto_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return inputs, labels, party\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.party_manifestos)\n",
    "\n",
    "dataset = PartyManifestoDataset(party_manifestos, tokenizer)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the language model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def chat_with_party_manifestos(user_query):\n",
    "    current_party = None\n",
    "\n",
    "    # Determine the party mentioned in the user's query\n",
    "    party = identify_party(user_query)\n",
    "\n",
    "    if party:\n",
    "        # If the identified party is different from the current party, load the manifesto\n",
    "        if party != current_party:\n",
    "            manifesto_text = load_manifesto_by_party(party)\n",
    "            if manifesto_text:\n",
    "                current_party = party\n",
    "            else:\n",
    "                return \"Chatbot: Sorry, I couldn't load the manifesto for that party.\"\n",
    "        else:\n",
    "            current_party = party\n",
    "\n",
    "        # Use the language model to generate a response based on the manifesto\n",
    "        input_prompt = f\"{manifesto_text}\\nYou: {user_query}\\nChatbot: \"\n",
    "        encoder_inputs = tokenizer.encode_plus(\n",
    "            input_prompt,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        decoder_input_ids = torch.tensor([[0]])  # Initialize with a single token (EOS)\n",
    "        # Set the top-k value\n",
    "        top_k = 50\n",
    "\n",
    "        # Generate responses using top-k sampling\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoder_inputs[\"input_ids\"],\n",
    "            attention_mask=encoder_inputs[\"attention_mask\"],\n",
    "            max_length=128,\n",
    "            do_sample=True,\n",
    "            top_k=top_k,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # Select the top response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return f\"Chatbot: {response}\"\n",
    "    else:\n",
    "        return \"Chatbot: Please specify which party you are referring to.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
